{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from helper import create_batches\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shelve\n",
    "import joblib\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "#Cuz the file is inside 'code' directory\n",
    "mount_point = \"../shelved_data/\"\n",
    "\n",
    "with shelve.open(mount_point+'IAM_Data') as shelf:\n",
    "    vocabulary = shelf['chars']\n",
    "    list_of_images = shelf['list_of_images']\n",
    "    image_labels = shelf['image_labels']\n",
    "    \n",
    "image_arrays = joblib.load(mount_point+'image_arrays')\n",
    "\n",
    "#List_images ko sort karo\n",
    "list_of_images.sort()\n",
    "\n",
    "#Convert vocabulary to list\n",
    "vocabulary = list(vocabulary)\n",
    "#Sort so as to have the same ordering every time..\n",
    "vocabulary.sort()\n",
    "vocabulary.append(\"<Blank>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "img_height = 104\n",
    "img_width = 688\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "#Common Hyper Parameters\n",
    "epochs = 700\n",
    "batch_size = 250 #Should be proportional to the number of Images\n",
    "\n",
    "#LSTM Params\n",
    "rnn_hidden_units = 200\n",
    "\n",
    "conv_out_height, conv_out_width = (int(math.ceil(img_height/(2**3 * 3))),int(math.ceil(img_width/(2**3 * 3))))\n",
    "\n",
    "#Number of time_steps to unroll for..\n",
    "seq_len = conv_out_height * conv_out_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save my MoDel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take first 12000 images...\n",
    "training_list = list_of_images[:12000]\n",
    "testing_list = list_of_images[12000:12250]\n",
    "random.shuffle(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_x,train_batches_y = create_batches(batch_size,training_list,image_arrays,image_labels,vocabulary)\n",
    "test_batches_x,test_batches_y = create_batches(len(testing_list),testing_list,image_arrays,image_labels,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_epoch = 620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/Lines_RNN_620\n",
      "620,102.244720,0.59,0.84,152.1252200603485\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_620 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "622,101.425079,0.63,0.91,153.6421046257019\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_622 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "624,102.806831,0.66,0.86,151.15894079208374\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_624 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "626,104.563675,0.70,0.84,149.40807247161865\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_626 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "628,101.582375,0.57,0.86,149.62976551055908\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_628 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "630,102.903816,0.60,0.90,149.43674278259277\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_630 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "632,100.351097,0.64,0.83,149.3182294368744\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_632 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "634,103.161064,0.64,0.81,149.16637182235718\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_634 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "636,100.233040,0.58,0.85,147.71416974067688\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_636 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "638,100.532761,0.62,0.82,147.80457019805908\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_638 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "642,99.039017,0.59,0.84,147.67674207687378\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_642 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "644,101.207802,0.63,0.84,147.57708501815796\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_644 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "646,102.694588,0.64,0.91,147.7575080394745\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_646 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "648,100.992203,0.60,0.84,147.69290208816528\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_648 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "650,106.536835,0.64,0.97,147.30089402198792\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_650 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "652,101.432800,0.60,0.91,147.38943099975586\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_652 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "654,103.427681,0.65,0.78,147.2132761478424\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_654 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "656,102.545525,0.61,0.89,146.73223781585693\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_656 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "658,100.349503,0.59,0.83,146.83171725273132\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_658 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "660,102.321121,0.59,0.82,146.63973879814148\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_660 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "662,100.791451,0.60,0.87,146.8458251953125\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_662 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "664,100.959160,0.58,0.85,146.7151861190796\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_664 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "666,101.240845,0.66,0.79,146.55551886558533\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_666 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "668,142.382599,0.75,1.00,146.69716620445251\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_668 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "670,136.685806,0.75,1.00,146.65734767913818\n",
      "\n",
      "INFO:tensorflow:../model/Lines_RNN_670 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Load the graph\n",
    "    saver = tf.train.import_meta_graph('../model/Lines_RNN_'+str(resume_epoch)+'.meta')\n",
    "\n",
    "    # Restore the weights and biases\n",
    "    saver.restore(sess, '../model/Lines_RNN_'+str(resume_epoch))\n",
    "\n",
    "    inputs = sess.graph.get_tensor_by_name('Placeholder:0')\n",
    "    \n",
    "    target_indices = sess.graph.get_tensor_by_name('targets/indices:0')\n",
    "    target_values = sess.graph.get_tensor_by_name('targets/values:0')\n",
    "    target_shape = sess.graph.get_tensor_by_name('targets/shape:0')\n",
    "\n",
    "    is_test = sess.graph.get_tensor_by_name('Placeholder_1:0')\n",
    "    iteration = sess.graph.get_tensor_by_name('Placeholder_2:0')\n",
    "    time_steps = sess.graph.get_tensor_by_name('Placeholder_3:0')\n",
    "    dropout_lstm = sess.graph.get_tensor_by_name('Placeholder_4:0')\n",
    "    dropout_fc = sess.graph.get_tensor_by_name('Placeholder_5:0')\n",
    "    \n",
    "        \n",
    "    train = sess.graph.get_operation_by_name('RMSProp')\n",
    "    update_ema = sess.graph.get_operation_by_name('group_deps')\n",
    "    cost = sess.graph.get_tensor_by_name('Mean:0')\n",
    "    label_error_rate = sess.graph.get_tensor_by_name('Mean_1:0')\n",
    "    \n",
    "    #For debugging purposes...\n",
    "    fc_outputs_2_bn = sess.graph.get_tensor_by_name('batchnorm_6/add_1:0')\n",
    "\n",
    "    #checkpoint flag\n",
    "    checkpoint = False\n",
    "    \n",
    "    timer = 0\n",
    "    \n",
    "    for e in range(resume_epoch,epochs): \n",
    "\n",
    "        start_time = time.time()\n",
    "    \n",
    "        #Checkpoint every 1 epochs\n",
    "        if (e%2)==0:\n",
    "            checkpoint = True\n",
    "\n",
    "        random.shuffle(training_list)\n",
    "        train_batches_x,train_batches_y = create_batches(batch_size,training_list,image_arrays,image_labels,vocabulary)\n",
    "\n",
    "        #Iterate through all images in a single epoch...\n",
    "        for b in range(len(train_batches_x)):\n",
    "            \n",
    "            feed_train = {\n",
    "                    inputs:train_batches_x[b].transpose([2,0,1]),\n",
    "                    target_indices:train_batches_y[b][0],target_values:train_batches_y[b][1],target_shape:train_batches_y[b][2],\n",
    "                    time_steps:np.array([seq_len]*batch_size),\n",
    "                    is_test:False,\n",
    "                    dropout_fc:np.array(0.7),dropout_lstm:np.array(0.8)\n",
    "                   }\n",
    "\n",
    "            feed_uma = {\n",
    "                    inputs:train_batches_x[b].transpose([2,0,1]),\n",
    "                    target_indices:train_batches_y[b][0],target_values:train_batches_y[b][1],target_shape:train_batches_y[b][2],\n",
    "                    time_steps:np.array([seq_len]*batch_size),\n",
    "                    is_test:False,iteration:len(train_batches_x)*e + b,\n",
    "                    dropout_fc:np.array(1.0),dropout_lstm:np.array(1.0)\n",
    "                   }\n",
    "\n",
    "        \n",
    "            _,debug_out = sess.run([train,fc_outputs_2_bn],feed_dict=feed_train)\n",
    "            sess.run(update_ema,feed_dict=feed_uma)\n",
    "            \n",
    "\n",
    "        if checkpoint:\n",
    "            last_cost,train_ler = sess.run([cost,label_error_rate],feed_dict=feed_train)\n",
    "\n",
    "        #After iterating through all batches..\n",
    "        test_batch_size = len(testing_list)\n",
    "      \n",
    "        feed_test = {\n",
    "            inputs:test_batches_x[0].transpose([2,0,1]),\n",
    "            target_indices:test_batches_y[0][0],target_values:test_batches_y[0][1],target_shape:test_batches_y[0][2],\n",
    "            time_steps:np.array([seq_len]*test_batch_size),\n",
    "            is_test:True,\n",
    "            dropout_fc:np.array(1.0),dropout_lstm:np.array(1.0)\n",
    "           }\n",
    "            \n",
    "\n",
    "        #Evaluate the model, and store every 5 epochs...\n",
    "        if checkpoint:\n",
    "\n",
    "            #Accuracy on test_data\n",
    "            ler_val = sess.run(label_error_rate,feed_dict=feed_test)                \n",
    "\n",
    "            end_time = time.time()       \n",
    "            time_taken = end_time - start_time\n",
    "            timer += time_taken\n",
    "\n",
    "            np.savez_compressed(str(e)+\"_fcouts2bn\",a=debug_out)\n",
    "\n",
    "            print(\"{},{:.6f},{:.2f},{:.2f},{}\\n\".format(e,last_cost,train_ler,ler_val,timer))\n",
    "#             np.savetxt(str(e)+\"_fcouts2bn.txt\",debug_out,delimiter=',')\n",
    "#             with open('debug.txt','a') as d:\n",
    "#                 d.write(debug_out)\n",
    "                \n",
    "            with open('progress.csv','a') as f:\n",
    "                f.write(\"{},{:.6f},{:.2f},{:.2f},{}\\n\".format(e,last_cost,train_ler,ler_val,timer))\n",
    "\n",
    "#             Save the model\n",
    "            saver.save(sess,'../model/Lines_RNN_'+str(e))\n",
    "\n",
    "            checkpoint = False\n",
    "            timer = 0\n",
    "        \n",
    "        else:\n",
    "            end_time = time.time()       \n",
    "            time_taken = end_time - start_time\n",
    "            timer += time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
