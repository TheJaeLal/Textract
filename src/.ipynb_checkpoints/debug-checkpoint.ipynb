{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c8dba681d0cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi_implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0m_USE_C_DESCRIPTORS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/google/protobuf/internal/api_implementation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_api_implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0;31m# The compile-time constants in the _api_implementation module can be used to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m# switch to a certain implementation of the Python API at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helper import create_batches\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shelve\n",
    "import joblib\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "#Cuz the file is inside 'code' directory\n",
    "mount_point = \"../shelved_data/\"\n",
    "\n",
    "with shelve.open(mount_point+'IAM_Data') as shelf:\n",
    "    vocabulary = shelf['chars']\n",
    "    list_of_images = shelf['list_of_images']\n",
    "    image_labels = shelf['image_labels']\n",
    "    \n",
    "image_arrays = joblib.load(mount_point+'image_arrays')\n",
    "\n",
    "#List_images ko sort karo\n",
    "list_of_images.sort()\n",
    "\n",
    "#Convert vocabulary to list\n",
    "vocabulary = list(vocabulary)\n",
    "#Sort so as to have the same ordering every time..\n",
    "vocabulary.sort()\n",
    "vocabulary.append(\"<Blank>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "img_height = 104\n",
    "img_width = 688\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "#Common Hyper Parameters\n",
    "epochs = 700\n",
    "batch_size = 250 #Should be proportional to the number of Images\n",
    "\n",
    "#LSTM Params\n",
    "rnn_hidden_units = 200\n",
    "\n",
    "conv_out_height, conv_out_width = (int(math.ceil(img_height/(2**3 * 3))),int(math.ceil(img_width/(2**3 * 3))))\n",
    "\n",
    "#Number of time_steps to unroll for..\n",
    "seq_len = conv_out_height * conv_out_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save my MoDel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take first 12000 images...\n",
    "training_list = list_of_images[:12000]\n",
    "testing_list = list_of_images[12000:12250]\n",
    "random.shuffle(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_x,train_batches_y = create_batches(batch_size,training_list,image_arrays,image_labels,vocabulary)\n",
    "test_batches_x,test_batches_y = create_batches(len(testing_list),testing_list,image_arrays,image_labels,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_epoch = 620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Load the graph\n",
    "    saver = tf.train.import_meta_graph('../model/Lines_RNN_'+str(resume_epoch)+'.meta')\n",
    "\n",
    "    # Restore the weights and biases\n",
    "    saver.restore(sess, '../model/Lines_RNN_'+str(resume_epoch))\n",
    "\n",
    "    inputs = sess.graph.get_tensor_by_name('Placeholder:0')\n",
    "    \n",
    "    target_indices = sess.graph.get_tensor_by_name('targets/indices:0')\n",
    "    target_values = sess.graph.get_tensor_by_name('targets/values:0')\n",
    "    target_shape = sess.graph.get_tensor_by_name('targets/shape:0')\n",
    "\n",
    "    is_test = sess.graph.get_tensor_by_name('Placeholder_1:0')\n",
    "    iteration = sess.graph.get_tensor_by_name('Placeholder_2:0')\n",
    "    time_steps = sess.graph.get_tensor_by_name('Placeholder_3:0')\n",
    "    dropout_lstm = sess.graph.get_tensor_by_name('Placeholder_4:0')\n",
    "    dropout_fc = sess.graph.get_tensor_by_name('Placeholder_5:0')\n",
    "    \n",
    "        \n",
    "    train = sess.graph.get_operation_by_name('RMSProp')\n",
    "    update_ema = sess.graph.get_operation_by_name('group_deps')\n",
    "    cost = sess.graph.get_tensor_by_name('Mean:0')\n",
    "    label_error_rate = sess.graph.get_tensor_by_name('Mean_1:0')\n",
    "    \n",
    "    #For debugging purposes...\n",
    "    fc_outputs_2_bn = sess.graph.get_tensor_by_name('batchnorm_6/add_1:0')\n",
    "\n",
    "    #checkpoint flag\n",
    "    checkpoint = False\n",
    "    \n",
    "    timer = 0\n",
    "    \n",
    "    for e in range(resume_epoch,epochs): \n",
    "\n",
    "        start_time = time.time()\n",
    "    \n",
    "        #Checkpoint every 1 epochs\n",
    "        if (e%2)==0:\n",
    "            checkpoint = True\n",
    "\n",
    "        random.shuffle(training_list)\n",
    "        train_batches_x,train_batches_y = create_batches(batch_size,training_list,image_arrays,image_labels,vocabulary)\n",
    "\n",
    "        #Iterate through all images in a single epoch...\n",
    "        for b in range(len(train_batches_x)):\n",
    "            \n",
    "            feed_train = {\n",
    "                    inputs:train_batches_x[b].transpose([2,0,1]),\n",
    "                    target_indices:train_batches_y[b][0],target_values:train_batches_y[b][1],target_shape:train_batches_y[b][2],\n",
    "                    time_steps:np.array([seq_len]*batch_size),\n",
    "                    is_test:False,\n",
    "                    dropout_fc:np.array(0.7),dropout_lstm:np.array(0.8)\n",
    "                   }\n",
    "\n",
    "            feed_uma = {\n",
    "                    inputs:train_batches_x[b].transpose([2,0,1]),\n",
    "                    target_indices:train_batches_y[b][0],target_values:train_batches_y[b][1],target_shape:train_batches_y[b][2],\n",
    "                    time_steps:np.array([seq_len]*batch_size),\n",
    "                    is_test:False,iteration:len(train_batches_x)*e + b,\n",
    "                    dropout_fc:np.array(1.0),dropout_lstm:np.array(1.0)\n",
    "                   }\n",
    "\n",
    "        \n",
    "            _,debug_out = sess.run([train,fc_outputs_2_bn],feed_dict=feed_train)\n",
    "            sess.run(update_ema,feed_dict=feed_uma)\n",
    "            \n",
    "\n",
    "        if checkpoint:\n",
    "            last_cost,train_ler = sess.run([cost,label_error_rate],feed_dict=feed_train)\n",
    "\n",
    "        #After iterating through all batches..\n",
    "        test_batch_size = len(testing_list)\n",
    "      \n",
    "        feed_test = {\n",
    "            inputs:test_batches_x[0].transpose([2,0,1]),\n",
    "            target_indices:test_batches_y[0][0],target_values:test_batches_y[0][1],target_shape:test_batches_y[0][2],\n",
    "            time_steps:np.array([seq_len]*test_batch_size),\n",
    "            is_test:True,\n",
    "            dropout_fc:np.array(1.0),dropout_lstm:np.array(1.0)\n",
    "           }\n",
    "            \n",
    "\n",
    "        #Evaluate the model, and store every 5 epochs...\n",
    "        if checkpoint:\n",
    "\n",
    "            #Accuracy on test_data\n",
    "            ler_val = sess.run(label_error_rate,feed_dict=feed_test)                \n",
    "\n",
    "            end_time = time.time()       \n",
    "            time_taken = end_time - start_time\n",
    "            timer += time_taken\n",
    "\n",
    "            np.savez_compressed(str(e)+\"_fcouts2bn\",a=debug_out)\n",
    "\n",
    "            print(\"{},{:.6f},{:.2f},{:.2f},{}\\n\".format(e,last_cost,train_ler,ler_val,timer))\n",
    "#             np.savetxt(str(e)+\"_fcouts2bn.txt\",debug_out,delimiter=',')\n",
    "#             with open('debug.txt','a') as d:\n",
    "#                 d.write(debug_out)\n",
    "                \n",
    "            with open('progress.csv','a') as f:\n",
    "                f.write(\"{},{:.6f},{:.2f},{:.2f},{}\\n\".format(e,last_cost,train_ler,ler_val,timer))\n",
    "\n",
    "#             Save the model\n",
    "            saver.save(sess,'../model/Lines_RNN_'+str(e))\n",
    "\n",
    "            checkpoint = False\n",
    "            timer = 0\n",
    "        \n",
    "        else:\n",
    "            end_time = time.time()       \n",
    "            time_taken = end_time - start_time\n",
    "            timer += time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
