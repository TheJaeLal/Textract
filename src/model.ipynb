{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78414c1204f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mArch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/hcr-ann/src/Arch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m FC = [\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m'units'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBRNN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_units'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'activate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;34m{\u001b[0m\u001b[0;34m'units'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'activate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m'units'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'activate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import layers\n",
    "from Arch import CNN, BRNN, FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Debugging flag..\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/gpu:0'):\n",
    "\n",
    "#Model\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "wconv,bconv,wfc,bfc = layers.init_weights(CNN,FC)\n",
    "\n",
    "dropout_conv = tf.placeholder(tf.float32,shape=[])\n",
    "dropout_lstm = tf.placeholder(tf.float32,shape=[])\n",
    "dropout_fc = tf.placeholder(tf.float32,shape=[])\n",
    "\n",
    "#Input 'Image'\n",
    "inputs = tf.placeholder(tf.float32,shape=[None,img_height,img_width])\n",
    "\n",
    "X = tf.reshape(inputs,(-1,img_height,img_width,1))\n",
    "\n",
    "#-------------------Convolution-----------------------#\n",
    "\n",
    "conv = [None] * len(CNN)\n",
    "\n",
    "#Create your CNN\n",
    "for i in range(len(CNN)):\n",
    "    strides = CNN[i]['conv'][1]\n",
    "    conv[i] = layers.conv(X,wconv[i],bconv[i],strides,CNN[i]['activate'],dropout_conv)\n",
    "    \n",
    "    if CNN[i]['pool']:\n",
    "        conv[i] = layers.max_pool(conv[i],CNN[i]['pool'])\n",
    "\n",
    "#--------All right upto here------------#\n",
    "\n",
    "#Calculate height and width of output from CNN\n",
    "conv_out_height,conv_out_width = layers.calc_out_dims(CNN,img_height,img_width)\n",
    "if debug:\n",
    "    print('Convolution_Output_size:({},{})'.format(conv_out_height,conv_out_width))\n",
    "\n",
    "#----------------LSTM--------------------------#\n",
    "#Treat a single pixel from each filter or feature map as an individual feature\n",
    "#So number of features  = num_conv4 filters or feature maps\n",
    "#length_of_sequence = width * height of the output from conv3 \n",
    "\n",
    "filters_in_last_conv = CNN[-1]['conv'][2]\n",
    "lstm_inputs = tf.reshape(conv[-1],(-1,conv_out_height*conv_out_width,filters_in_last_conv))\n",
    "\n",
    "#Number of time_steps to unroll for..\n",
    "seq_len = conv_out_height * conv_out_width\n",
    "\n",
    "#So that we can use different batch size during testing...\n",
    "time_steps = tf.placeholder(tf.int32,shape = [None])\n",
    "targets = tf.sparse_placeholder(tf.int32,name='targets')\n",
    "\n",
    "lstm_initializer = tf.contrib.layers.xavier_initializer()\n",
    "fw_layer = layers.lstm(BRNN['layers'],BRNN['hidden_units'],lstm_initializer,dropout=dropout_lstm)\n",
    "bw_layer = layers.lstm(BRNN['layers'],BRNN['hidden_units'],lstm_initializer,dropout=dropout_lstm)\n",
    "(outputs_fw,outputs_bw),_ = tf.nn.bidirectional_dynamic_rnn(fw_layer,bw_layer,lstm_inputs,dtype=tf.float32)\n",
    "\n",
    "# outputs,_,_ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw,cells_bw,inputs = lstm_inputs,dtype=tf.float32)\n",
    "if debug:\n",
    "    print('LSTM_Output_size:({},{})'.format(outputs_fw,outputs_bw))\n",
    "\n",
    "#Concatenate the output from both cells (forward and backward)\n",
    "blstm_outputs = tf.concat([outputs_fw,outputs_bw], 2)\n",
    "\n",
    "#flatten out all except the last dimension\n",
    "fc_inputs  = tf.reshape(blstm_outputs,[-1,2*rnn_hidden_units])\n",
    "\n",
    "#Feed into the fully connected layer\n",
    "#No activation cuz, the output of this layer is feeded into CTC Layer as logits\n",
    "fc_outputs_1 = layers.fc(fc_inputs,wfc1,bfc1,activation=None,dropout=dropout_fc)\n",
    "fc_outputs_2 = layers.fc(fc_outputs_1,wfc2,bfc2,activation=None,dropout=dropout_fc)\n",
    "\n",
    "#Reshape back to batch_size, seq_len,vocab_size\n",
    "logits = tf.reshape(fc_outputs_2,[-1,seq_len,vocab_size])\n",
    "\n",
    "#convert them to time major\n",
    "logits = tf.transpose(logits,[1,0,2])\n",
    "\n",
    "#Calculate loss\n",
    "loss = tf.nn.ctc_loss(targets, logits, time_steps)\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "#Optimize\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=alpha)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# CTC decoder.\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, time_steps)\n",
    "label_error_rate = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
